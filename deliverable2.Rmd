---
title: "School Revenue Data Science Project"
author: "Brenndan Olson"
output: html_document
---

To do:
Add commentary
clean data and import deliverable1
add markdown formatting
do model and visualization
add to index.html
make sure its litty

```{r}
library(rvest)
library(tidyverse)
library(dplyr)
library(caret)
library(knitr)
purl("deliverable1.Rmd", output = "part1.r")
source("part1.r")
```
To get an additional data source I am going to web scrape from a website called schooldigger.com that contains rankings for each school district within their respective state. 
```{r}
html_link <- "https://www.schooldigger.com/go/AL/districtrank.aspx?y=2018" 
html <- read_html(html_link)
```
Inside the html link we gave it, we are going to look for nodes under the name "table"
```{r}
tabl <- html %>% html_nodes("table")
t <- html_table(tabl)
```
We are going to use that scraped data to make another tibble called district_rankings_2017. The only two columns we need are the school district and rank, and we are also going to omit any school districts that don't have a ranking. While there is potentially a lot to be explored with unranked districts, it is hard to compare them against each other, and they do not provide much support for the model we are going to build later.
```{r}  
district_rankings_2017 <- dplyr::tibble(
  school_district = t[[1]]$District,
  rank = t[[1]]$`Rank (2017)`)

district_rankings_2017 <- district_rankings_2017 %>%
  na.omit(cols = "rank")
```
To make sure that both datasets have the same name and format for school districts, I am going to modify the school_district column of district_rankings_2017 to append "School District" to every district name, and make every character uppercase to match how they are written in the school_districts table
```{r}
district_rankings_2017$school_district <- 
  lapply(district_rankings_2017$school_district, function(x) paste(x,'School District')) %>%
  sapply(toupper)
head(district_rankings_2017$school_district)
```
Since we are only focusing on one state, I am going to create a new table filtered with the state we are using. Using that, we can left join with district_rankings_2017 and add some of the variables we are going to use to build a predictive model.
```{r}
school_districts_alabama <- school_districts %>%
  dplyr::filter(state == "Alabama")
state_revenue_alabama <- state_revenue %>%
  dplyr::filter(state == "Alabama")

districts_with_rankings_2017 <- left_join(school_districts_alabama,district_rankings_2017, by = "school_district") 
districts_with_rankings_2017 <- districts_with_rankings_2017 %>%
  dplyr::mutate(state_rev = state_revenue_alabama$total_state_revenue) %>%
  dplyr::mutate(general_formula_assistance = state_revenue_alabama$general_formula_assistance) %>%
  na.omit(cols = "rank")

```

Modeling 
With our new data that contains the rankings for each school, I am curious if there is a way to predict the ranking of the district, based on some of the information gathered on state revenue.

To create a predictive model, we want to have our data split into two tables, the training table and the test table. The training table contains 75% of the data, and is used for making the model. The testing table has the remaining 25% of data and is used to assess the model's precision. To randomly separate the data there is a neat function called createDataPartition().
```{r}
index <- createDataPartition(districts_with_rankings_2017$rank, p = 0.75, list = FALSE)
train <- districts_with_rankings_2017[index,]
test <- districts_with_rankings_2017[-index,]
```
Now that we have everything set up, we can get into the modeling. Like I said, we are going to use the training data to create this model, and for this first one we are going to try and predict district rank based on total state revenue, fall attendance, and general formaula assistance.
```{r}
model <- lm(train, formula = rank~state_rev + fall_attendance + general_formula_assistance)
summary(model)
```

```{r}
model <- lm(train, formula = rank~state_rev + general_formula_assistance)
summary(model)
```
One limitation of this model is that part of the p-values are dependent on which data is in the test and train table, because if you run the createDataPartition() part and then re-run the model, you get different p-values.

Now I am going to use the coefficients given in the model to create a vector of what the model predicted the rankings to be.
```{r}
test_model <- function(model)
{
  modeled_rankings <- c()
  for(i in 1:nrow(test))
  {
    yhat <- model$coefficients[1] + test$state_rev[i]*model$coefficients[2] + test$general_formula_assistance[i]*model$coefficients[3]
    modeled_rankings <- c(modeled_rankings,yhat)
  }
  return(modeled_rankings)
}
modeled_avg_rankings <- test_model(model)
test_avg_rankings <- test$rank
```
I am now going to graph the results of the model, and compare them to the actual rankings in the testing table
```{r}
plot(x=1:length(test_avg_rankings),y=sort(test_avg_rankings),type="p",col="red")
  points(x=1:length(modeled_avg_rankings),y=sort(modeled_avg_rankings))
  title(main = "Model average_ratings(black) vs data average_ratings(red)")
```
As you can see the model is not very accurate. I already stated the limitation about variation on the model's results, but there are a few others. One of the other biggest factors about this model is it does not account for size of a district, because generally a larger district would receive more state funding. However since the fall_attendance column was negatively affecting the model, I decided to remove it. Another limitation is that 